{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5afe5b",
   "metadata": {},
   "source": [
    "# Distill `MongoDB/mdbr-leaf-mt` â†’ OpenAI `text-embedding-3-large` (1536-d)\n",
    "\n",
    "**Goal**: Reproduce [Kabumbus/mdbr-leaf-mt-1536-onnx](https://huggingface.co/Kabumbus/mdbr-leaf-mt-1536-onnx) but targeting `text-embedding-3-large` embeddings instead of `text-embedding-3-small`, using the [Qdrant/dbpedia-entities-openai3-text-embedding-3-large-1536-1M](https://huggingface.co/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-1536-1M) dataset.\n",
    "\n",
    "**Architecture**:\n",
    "- **Student**: `MongoDB/mdbr-leaf-mt` (23M params, BERT-based SentenceTransformer)\n",
    "- **Teacher labels**: Pre-computed OpenAI `text-embedding-3-large` embeddings (1536-d)\n",
    "- **Loss**: Cosine regression â€” `loss = 1 - mean(cosine_similarity(student, teacher))`\n",
    "- **Partial fine-tuning**: Only the Dense projection head + last 2 transformer blocks are trainable\n",
    "\n",
    "**Resilience**: All checkpoints, processed data, and training state are saved to Google Drive, so training survives Colab disconnections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e9d7d",
   "metadata": {},
   "source": [
    "## 0. Setup & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f87e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers datasets transformers accelerate onnx onnxruntime optimum[onnxruntime] matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266f1d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Mount Google Drive for persistent storage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "import os, json, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# All persistent artefacts live here\n",
    "DRIVE_ROOT = Path(\"/content/drive/MyDrive/distil_mdbr_large_1536\")\n",
    "DRIVE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_DIR = DRIVE_ROOT / \"checkpoints\"\n",
    "PROCESSED_DATA = DRIVE_ROOT / \"processed_dataset\"\n",
    "FINAL_MODEL_DIR = DRIVE_ROOT / \"final_model\"\n",
    "ONNX_DIR = DRIVE_ROOT / \"onnx_export\"\n",
    "STATE_FILE = DRIVE_ROOT / \"training_state.json\"\n",
    "\n",
    "for d in [CHECKPOINT_DIR, PROCESSED_DATA, FINAL_MODEL_DIR, ONNX_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"âœ… Drive mounted. Working directory: {DRIVE_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8566a363",
   "metadata": {},
   "source": [
    "## 1. Load & Prepare Dataset\n",
    "\n",
    "The dataset has columns: `_id`, `title`, `text`, `openai` (the 1536-d teacher embedding).\n",
    "We combine `title + \" \" + text` into a single `text` field and rename `openai` â†’ `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1a5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk, Features, Value, Sequence\n",
    "import numpy as np\n",
    "\n",
    "# â”€â”€ Check if we already prepared & saved the dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if (PROCESSED_DATA / \"dataset_dict.json\").exists():\n",
    "    print(\"ğŸ“‚ Loading pre-processed dataset from Driveâ€¦\")\n",
    "    from datasets import DatasetDict\n",
    "    ds = DatasetDict.load_from_disk(str(PROCESSED_DATA))\n",
    "    print(f\"   Train: {len(ds['train']):,}  |  Eval: {len(ds['test']):,}\")\n",
    "else:\n",
    "    print(\"â¬‡ï¸  Downloading Qdrant/dbpedia-entities-openai3-text-embedding-3-large-1536-1M â€¦\")\n",
    "    raw = load_dataset(\n",
    "        \"Qdrant/dbpedia-entities-openai3-text-embedding-3-large-1536-1M\",\n",
    "        split=\"train\",\n",
    "    )\n",
    "    print(f\"   Raw rows: {len(raw):,}\")\n",
    "    print(f\"   Columns : {raw.column_names}\")\n",
    "    print(f\"   Example keys: {list(raw[0].keys())}\")\n",
    "\n",
    "    # â”€â”€ Combine title + text â†’ single 'text' field; rename openai â†’ labels â”€\n",
    "    def preprocess(example):\n",
    "        title = (example.get(\"title\") or \"\").strip()\n",
    "        body  = (example.get(\"text\")  or \"\").strip()\n",
    "        combined = f\"{title} {body}\".strip() if title else body\n",
    "        return {\"text\": combined, \"labels\": example[\"openai\"]}\n",
    "\n",
    "    raw = raw.map(preprocess, remove_columns=raw.column_names, num_proc=4,\n",
    "                  desc=\"Preprocessing\")\n",
    "\n",
    "    # Cast labels to fixed-length float32 sequence\n",
    "    new_features = Features({\n",
    "        \"text\":   Value(\"string\"),\n",
    "        \"labels\": Sequence(Value(\"float32\"), length=1536),\n",
    "    })\n",
    "    raw = raw.cast(new_features)\n",
    "\n",
    "    # â”€â”€ Shuffle & split: 99 % train / 1 % eval â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    raw = raw.shuffle(seed=42)\n",
    "    split = raw.train_test_split(test_size=0.01, seed=42)\n",
    "\n",
    "    print(f\"   Train: {len(split['train']):,}  |  Eval: {len(split['test']):,}\")\n",
    "\n",
    "    # â”€â”€ Persist to Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    split.save_to_disk(str(PROCESSED_DATA))\n",
    "    ds = split\n",
    "    print(\"ğŸ’¾ Dataset saved to Drive.\")\n",
    "\n",
    "print(f\"\\nâœ… Dataset ready.  Sample text[:120]: {ds['train'][0]['text'][:120]}â€¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5a73d0",
   "metadata": {},
   "source": [
    "## 2. Load Base Model & Replace Projection Head\n",
    "\n",
    "Load `MongoDB/mdbr-leaf-mt` and replace (or append) the Dense projection head to output **1536** dimensions with Xavier-uniform init. Then freeze everything except:\n",
    "- The Dense head(s)\n",
    "- The last 2 transformer blocks\n",
    "- Any final normalisation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af258cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.models import Dense\n",
    "\n",
    "TARGET_DIM = 1536\n",
    "BASE_MODEL_NAME = \"MongoDB/mdbr-leaf-mt\"\n",
    "\n",
    "# â”€â”€ Load base model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model = SentenceTransformer(BASE_MODEL_NAME)\n",
    "print(\"Base model modules:\")\n",
    "for i, mod in enumerate(model):\n",
    "    print(f\"  [{i}] {mod.__class__.__name__}  â†’  {type(mod)}\")\n",
    "\n",
    "# â”€â”€ Determine the backbone hidden size â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# The Transformer module's output dim is the backbone hidden size\n",
    "backbone = model[0]  # Transformer module\n",
    "hidden_size = backbone.get_word_embedding_dimension()\n",
    "print(f\"\\nBackbone hidden size: {hidden_size}\")\n",
    "\n",
    "# â”€â”€ Replace / append Dense head to project to 1536 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Check if a Dense layer already exists\n",
    "dense_idx = None\n",
    "for i, mod in enumerate(model):\n",
    "    if isinstance(mod, Dense):\n",
    "        dense_idx = i\n",
    "        break\n",
    "\n",
    "new_dense = Dense(\n",
    "    in_features=hidden_size,\n",
    "    out_features=TARGET_DIM,\n",
    "    bias=False,\n",
    "    activation_function=nn.Identity(),\n",
    ")\n",
    "# Xavier uniform initialisation\n",
    "nn.init.xavier_uniform_(new_dense.linear.weight)\n",
    "\n",
    "if dense_idx is not None:\n",
    "    print(f\"â™»ï¸  Replacing existing Dense at index {dense_idx} ({hidden_size} â†’ {TARGET_DIM})\")\n",
    "    model[dense_idx] = new_dense\n",
    "else:\n",
    "    print(f\"â• Appending new Dense head ({hidden_size} â†’ {TARGET_DIM})\")\n",
    "    model.append(new_dense)\n",
    "\n",
    "# â”€â”€ Freeze all, then selectively unfreeze â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 1) Unfreeze Dense head(s)\n",
    "for mod in model:\n",
    "    if isinstance(mod, Dense):\n",
    "        for param in mod.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "# 2) Unfreeze last 2 transformer encoder blocks\n",
    "encoder = backbone.auto_model\n",
    "# Works for BERT-style models (encoder.layer is a ModuleList)\n",
    "if hasattr(encoder, \"encoder\") and hasattr(encoder.encoder, \"layer\"):\n",
    "    layers = encoder.encoder.layer\n",
    "elif hasattr(encoder, \"layer\"):\n",
    "    layers = encoder.layer\n",
    "else:\n",
    "    raise RuntimeError(f\"Cannot find transformer layers in {type(encoder)}\")\n",
    "\n",
    "num_layers = len(layers)\n",
    "for layer in layers[-2:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "print(f\"ğŸ”“ Unfroze last 2 of {num_layers} transformer blocks\")\n",
    "\n",
    "# 3) Unfreeze any final LayerNorm / Normalize module\n",
    "for mod in model:\n",
    "    mod_name = mod.__class__.__name__.lower()\n",
    "    if \"norm\" in mod_name:\n",
    "        for param in mod.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(f\"ğŸ”“ Unfroze normalisation module: {mod.__class__.__name__}\")\n",
    "\n",
    "# Enable gradient checkpointing on the backbone\n",
    "if hasattr(encoder, \"gradient_checkpointing_enable\"):\n",
    "    encoder.gradient_checkpointing_enable()\n",
    "    print(\"âœ… Gradient checkpointing enabled\")\n",
    "\n",
    "# â”€â”€ Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "total  = sum(p.numel() for p in model.parameters())\n",
    "train  = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen = total - train\n",
    "print(f\"\\nTotal params:     {total:>12,}\")\n",
    "print(f\"Trainable params: {train:>12,}  ({100*train/total:.1f}%)\")\n",
    "print(f\"Frozen params:    {frozen:>12,}  ({100*frozen/total:.1f}%)\")\n",
    "print(f\"Output dim:       {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed0208",
   "metadata": {},
   "source": [
    "## 3. Training with Cosine Regression Loss\n",
    "\n",
    "**Loss**: `1 âˆ’ mean(cosine_similarity(student_emb, teacher_emb))`\n",
    "\n",
    "**Key hyperparameters** (matching the reference):\n",
    "- Epochs: 10\n",
    "- LR: 1e-3 (cosine schedule, 3% warmup)\n",
    "- Weight decay: 0.01\n",
    "- Optimizer: AdamW (fused)\n",
    "- BF16/FP16 auto-selection\n",
    "- Checkpoints saved to Drive every 500 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598c9687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import time, math, glob\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Custom Dataset wrapper\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "class EmbeddingDistillDataset(Dataset):\n",
    "    \"\"\"Wraps an HF dataset with columns 'text' and 'labels'.\"\"\"\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset[idx]\n",
    "        return row[\"text\"], torch.tensor(row[\"labels\"], dtype=torch.float32)\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Collate function â€” tokenise a batch of texts\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    encoded = tokenizer(\n",
    "        list(texts),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return encoded, torch.stack(labels)\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Cosine regression loss\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def cosine_loss(student_emb, teacher_emb):\n",
    "    \"\"\"1 âˆ’ mean(cosine_similarity(student, teacher))\"\"\"\n",
    "    s = F.normalize(student_emb, p=2, dim=-1)\n",
    "    t = F.normalize(teacher_emb, p=2, dim=-1)\n",
    "    return 1.0 - (s * t).sum(dim=-1).mean()\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Hyperparameters\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "EPOCHS        = 10\n",
    "BATCH_SIZE    = 64        # Adjust for GPU memory (T4=64, A100=256)\n",
    "LR            = 1e-3\n",
    "WEIGHT_DECAY  = 0.01\n",
    "WARMUP_RATIO  = 0.03\n",
    "SAVE_STEPS    = 500       # Checkpoint every N steps\n",
    "LOG_STEPS     = 50        # Log every N steps\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "# Auto-select precision\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_bf16 = device.type == \"cuda\" and torch.cuda.is_bf16_supported()\n",
    "use_fp16 = device.type == \"cuda\" and not use_bf16\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=use_fp16)\n",
    "amp_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "print(f\"Device: {device}  |  AMP dtype: {amp_dtype}  |  Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# DataLoaders\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "train_dataset = EmbeddingDistillDataset(ds[\"train\"])\n",
    "eval_dataset  = EmbeddingDistillDataset(ds[\"test\"])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    collate_fn=collate_fn, num_workers=2, pin_memory=True, drop_last=True,\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset, batch_size=BATCH_SIZE * 2, shuffle=False,\n",
    "    collate_fn=collate_fn, num_workers=2, pin_memory=True,\n",
    ")\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "total_steps = steps_per_epoch * EPOCHS\n",
    "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "print(f\"Steps/epoch: {steps_per_epoch:,}  |  Total steps: {total_steps:,}  |  Warmup: {warmup_steps:,}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Optimizer & Scheduler\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "try:\n",
    "    optimizer = torch.optim.AdamW(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, fused=True)\n",
    "    print(\"Using fused AdamW\")\n",
    "except TypeError:\n",
    "    optimizer = torch.optim.AdamW(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    print(\"Using standard AdamW (fused not available)\")\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Resume from checkpoint if available\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "start_epoch = 0\n",
    "global_step = 0\n",
    "train_losses = []\n",
    "eval_losses  = []\n",
    "\n",
    "def find_latest_checkpoint():\n",
    "    \"\"\"Find the latest checkpoint directory on Drive.\"\"\"\n",
    "    pattern = str(CHECKPOINT_DIR / \"step_*\")\n",
    "    ckpts = sorted(glob.glob(pattern), key=lambda x: int(x.split(\"step_\")[-1]))\n",
    "    return Path(ckpts[-1]) if ckpts else None\n",
    "\n",
    "latest_ckpt = find_latest_checkpoint()\n",
    "if latest_ckpt is not None:\n",
    "    print(f\"\\nğŸ”„ Resuming from checkpoint: {latest_ckpt.name}\")\n",
    "    ckpt = torch.load(latest_ckpt / \"training_state.pt\", map_location=device, weights_safe_only=False)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n",
    "    if use_fp16:\n",
    "        scaler.load_state_dict(ckpt[\"scaler_state_dict\"])\n",
    "    start_epoch  = ckpt[\"epoch\"]\n",
    "    global_step  = ckpt[\"global_step\"]\n",
    "    train_losses = ckpt.get(\"train_losses\", [])\n",
    "    eval_losses  = ckpt.get(\"eval_losses\", [])\n",
    "    print(f\"   Resumed at epoch {start_epoch}, global step {global_step}\")\n",
    "else:\n",
    "    print(\"\\nğŸ†• Starting training from scratch.\")\n",
    "\n",
    "model.to(device)\n",
    "print(\"âœ… Model moved to device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fdcc4e",
   "metadata": {},
   "source": [
    "### 3b. Training Loop (interrupt-safe)\n",
    "\n",
    "Every `SAVE_STEPS` steps the full training state (model, optimizer, scheduler, scaler, loss history) is saved to Google Drive. If the Colab runtime disconnects, just re-run all cells â€” training resumes from the last checkpoint automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb8f8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Evaluation helper\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss, n_batches = 0.0, 0\n",
    "    for encoded, labels in eval_loader:\n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "        labels = labels.to(device)\n",
    "        with torch.amp.autocast(\"cuda\", dtype=amp_dtype, enabled=(device.type == \"cuda\")):\n",
    "            output = model(encoded)\n",
    "            emb = output[\"sentence_embedding\"]\n",
    "            loss = cosine_loss(emb, labels)\n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    model.train()\n",
    "    return total_loss / max(n_batches, 1)\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Checkpoint saver\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def save_checkpoint(epoch, step, extra_tag=\"\"):\n",
    "    tag = f\"step_{step}\" + (f\"_{extra_tag}\" if extra_tag else \"\")\n",
    "    ckpt_path = CHECKPOINT_DIR / tag\n",
    "    ckpt_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"global_step\": step,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "        \"scaler_state_dict\": scaler.state_dict() if use_fp16 else {},\n",
    "        \"train_losses\": train_losses,\n",
    "        \"eval_losses\": eval_losses,\n",
    "    }\n",
    "    torch.save(state, ckpt_path / \"training_state.pt\")\n",
    "\n",
    "    # Keep only last 3 checkpoints to save space\n",
    "    all_ckpts = sorted(glob.glob(str(CHECKPOINT_DIR / \"step_*\")),\n",
    "                       key=lambda x: int(x.split(\"step_\")[-1].split(\"_\")[0]))\n",
    "    for old in all_ckpts[:-3]:\n",
    "        shutil.rmtree(old, ignore_errors=True)\n",
    "\n",
    "    print(f\"   ğŸ’¾ Checkpoint saved: {tag}\")\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Training loop\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"  TRAINING START  â€”  Epochs {start_epoch}â†’{EPOCHS}  |  Global step: {global_step}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "model.train()\n",
    "t0 = time.time()\n",
    "running_loss = 0.0\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    epoch_step = 0\n",
    "    for batch_idx, (encoded, labels) in enumerate(train_loader):\n",
    "        # Skip already-completed steps within the current epoch when resuming\n",
    "        current_abs_step = epoch * steps_per_epoch + batch_idx\n",
    "        if current_abs_step < global_step:\n",
    "            continue\n",
    "\n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.amp.autocast(\"cuda\", dtype=amp_dtype, enabled=(device.type == \"cuda\")):\n",
    "            output = model(encoded)\n",
    "            emb = output[\"sentence_embedding\"]\n",
    "            loss = cosine_loss(emb, labels)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if use_fp16:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(trainable_params, MAX_GRAD_NORM)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(trainable_params, MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        global_step += 1\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # â”€â”€ Logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        if global_step % LOG_STEPS == 0:\n",
    "            avg = running_loss / LOG_STEPS\n",
    "            lr_now = scheduler.get_last_lr()[0]\n",
    "            elapsed = time.time() - t0\n",
    "            steps_sec = LOG_STEPS / elapsed if elapsed > 0 else 0\n",
    "            cos_sim = 1.0 - avg\n",
    "            train_losses.append((global_step, avg))\n",
    "            print(\n",
    "                f\"  Epoch {epoch+1}/{EPOCHS}  \"\n",
    "                f\"Step {global_step:>7,}/{total_steps:,}  \"\n",
    "                f\"Loss: {avg:.4f}  \"\n",
    "                f\"CosSim: {cos_sim:.4f}  \"\n",
    "                f\"LR: {lr_now:.2e}  \"\n",
    "                f\"Speed: {steps_sec:.1f} step/s\"\n",
    "            )\n",
    "            running_loss = 0.0\n",
    "            t0 = time.time()\n",
    "\n",
    "        # â”€â”€ Checkpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        if global_step % SAVE_STEPS == 0:\n",
    "            eval_loss = evaluate()\n",
    "            eval_cos = 1.0 - eval_loss\n",
    "            eval_losses.append((global_step, eval_loss))\n",
    "            print(f\"   ğŸ“Š Eval loss: {eval_loss:.4f}  |  Eval CosSim: {eval_cos:.4f}\")\n",
    "            save_checkpoint(epoch, global_step)\n",
    "            model.train()\n",
    "            t0 = time.time()  # reset timer after checkpoint overhead\n",
    "\n",
    "    # â”€â”€ End-of-epoch eval & checkpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    eval_loss = evaluate()\n",
    "    eval_cos = 1.0 - eval_loss\n",
    "    eval_losses.append((global_step, eval_loss))\n",
    "    print(f\"\\nâœ… Epoch {epoch+1} complete  |  Eval loss: {eval_loss:.4f}  |  CosSim: {eval_cos:.4f}\")\n",
    "    save_checkpoint(epoch + 1, global_step, extra_tag=f\"epoch{epoch+1}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"  TRAINING COMPLETE  â€”  Final global step: {global_step}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b464a349",
   "metadata": {},
   "source": [
    "## 4. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0e99da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# â”€â”€ Loss curves â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if train_losses:\n",
    "    steps_t, losses_t = zip(*train_losses)\n",
    "    axes[0].plot(steps_t, losses_t, label=\"Train loss\", alpha=0.7)\n",
    "if eval_losses:\n",
    "    steps_e, losses_e = zip(*eval_losses)\n",
    "    axes[0].plot(steps_e, losses_e, \"o-\", label=\"Eval loss\", markersize=4)\n",
    "axes[0].set_xlabel(\"Global Step\")\n",
    "axes[0].set_ylabel(\"Cosine Loss\")\n",
    "axes[0].set_title(\"Loss Curves\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# â”€â”€ Cosine similarity curves â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if train_losses:\n",
    "    axes[1].plot(steps_t, [1 - l for l in losses_t], label=\"Train CosSim\", alpha=0.7)\n",
    "if eval_losses:\n",
    "    axes[1].plot(steps_e, [1 - l for l in losses_e], \"o-\", label=\"Eval CosSim\", markersize=4)\n",
    "axes[1].set_xlabel(\"Global Step\")\n",
    "axes[1].set_ylabel(\"Cosine Similarity\")\n",
    "axes[1].set_title(\"Student â†” Teacher Cosine Similarity\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(DRIVE_ROOT / \"training_curves.png\"), dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"ğŸ“ˆ Curves saved to {DRIVE_ROOT / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84f7714",
   "metadata": {},
   "source": [
    "## 5. Save Final SentenceTransformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f482998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final fine-tuned SentenceTransformer to Drive\n",
    "model.save(str(FINAL_MODEL_DIR))\n",
    "\n",
    "# Write export metadata\n",
    "export_meta = {\n",
    "    \"base_model\": BASE_MODEL_NAME,\n",
    "    \"teacher\": \"text-embedding-3-large\",\n",
    "    \"output_dim\": TARGET_DIM,\n",
    "    \"max_length\": 512,\n",
    "    \"normalized_output\": True,\n",
    "    \"dataset\": \"Qdrant/dbpedia-entities-openai3-text-embedding-3-large-1536-1M\",\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"lr\": LR,\n",
    "    \"final_eval_loss\": eval_losses[-1][1] if eval_losses else None,\n",
    "    \"final_eval_cossim\": 1 - eval_losses[-1][1] if eval_losses else None,\n",
    "}\n",
    "with open(FINAL_MODEL_DIR / \"export_meta.json\", \"w\") as f:\n",
    "    json.dump(export_meta, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Final model saved to: {FINAL_MODEL_DIR}\")\n",
    "print(f\"   export_meta.json: {json.dumps(export_meta, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2f3a83",
   "metadata": {},
   "source": [
    "## 6. Export to ONNX + INT8 Quantisation\n",
    "\n",
    "Export the trained model in three formats:\n",
    "1. **`model.onnx`** â€” Full-precision float32\n",
    "2. **`model.int8.onnx`** â€” Dynamic INT8 quantised (smaller, faster on CPU/WASM)\n",
    "3. **`model.int8.ort`** â€” ORT-format INT8 for minimal runtime deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5aa196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "from pathlib import Path\n",
    "\n",
    "onnx_float_path = ONNX_DIR / \"model.onnx\"\n",
    "onnx_int8_path  = ONNX_DIR / \"model.int8.onnx\"\n",
    "onnx_ort_path   = ONNX_DIR / \"model.int8.ort\"\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 6a. Export to ONNX (float32)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "\n",
    "# Create dummy inputs\n",
    "dummy_text = \"This is a test sentence for ONNX export.\"\n",
    "dummy_encoded = tokenizer(\n",
    "    [dummy_text], padding=\"max_length\", truncation=True,\n",
    "    max_length=512, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Build a wrapper that takes flat tensors and returns sentence_embedding\n",
    "class OnnxWrapper(nn.Module):\n",
    "    def __init__(self, st_model):\n",
    "        super().__init__()\n",
    "        self.st_model = st_model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        features = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "        if token_type_ids is not None:\n",
    "            features[\"token_type_ids\"] = token_type_ids\n",
    "        out = self.st_model(features)\n",
    "        emb = out[\"sentence_embedding\"]\n",
    "        # L2 normalise\n",
    "        return F.normalize(emb, p=2, dim=-1)\n",
    "\n",
    "wrapper = OnnxWrapper(model)\n",
    "wrapper.eval()\n",
    "\n",
    "# Determine input names and dummy inputs\n",
    "input_names = [\"input_ids\", \"attention_mask\"]\n",
    "dummy_args = (dummy_encoded[\"input_ids\"], dummy_encoded[\"attention_mask\"])\n",
    "if \"token_type_ids\" in dummy_encoded:\n",
    "    input_names.append(\"token_type_ids\")\n",
    "    dummy_args = dummy_args + (dummy_encoded[\"token_type_ids\"],)\n",
    "\n",
    "dynamic_axes = {name: {0: \"batch\", 1: \"seq\"} for name in input_names}\n",
    "dynamic_axes[\"sentence_embedding\"] = {0: \"batch\"}\n",
    "\n",
    "torch.onnx.export(\n",
    "    wrapper,\n",
    "    dummy_args,\n",
    "    str(onnx_float_path),\n",
    "    input_names=input_names,\n",
    "    output_names=[\"sentence_embedding\"],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=14,\n",
    "    do_constant_folding=True,\n",
    ")\n",
    "print(f\"âœ… Float ONNX exported: {onnx_float_path}\")\n",
    "print(f\"   Size: {onnx_float_path.stat().st_size / 1e6:.1f} MB\")\n",
    "\n",
    "# Validate\n",
    "onnx_model = onnx.load(str(onnx_float_path))\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"   ONNX model validated âœ“\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 6b. Dynamic INT8 quantisation\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "quantize_dynamic(\n",
    "    model_input=str(onnx_float_path),\n",
    "    model_output=str(onnx_int8_path),\n",
    "    weight_type=QuantType.QInt8,\n",
    ")\n",
    "print(f\"\\nâœ… INT8 ONNX exported: {onnx_int8_path}\")\n",
    "print(f\"   Size: {onnx_int8_path.stat().st_size / 1e6:.1f} MB\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 6c. Convert INT8 ONNX to ORT format\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "try:\n",
    "    from onnxruntime.transformers.convert_to_ort import convert_onnx_to_ort\n",
    "    convert_onnx_to_ort(str(onnx_int8_path))\n",
    "    # The .ort file is created alongside the .onnx file\n",
    "    generated_ort = onnx_int8_path.with_suffix(\".ort\")\n",
    "    if generated_ort.exists():\n",
    "        shutil.copy2(str(generated_ort), str(onnx_ort_path))\n",
    "        print(f\"\\nâœ… INT8 ORT exported: {onnx_ort_path}\")\n",
    "        print(f\"   Size: {onnx_ort_path.stat().st_size / 1e6:.1f} MB\")\n",
    "    else:\n",
    "        print(\"âš ï¸  ORT conversion completed but output file not found at expected path.\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  ORT format conversion not available. Skipping .ort export.\")\n",
    "    print(\"   Install with: pip install onnxruntime-tools\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Copy tokenizer files to ONNX directory for self-contained deployment\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "tokenizer_files = [\"tokenizer.json\", \"tokenizer_config.json\", \"vocab.txt\",\n",
    "                   \"special_tokens_map.json\", \"config.json\"]\n",
    "for fname in tokenizer_files:\n",
    "    src = FINAL_MODEL_DIR / fname\n",
    "    if not src.exists():\n",
    "        # Try inside the transformer subfolder\n",
    "        for sub in FINAL_MODEL_DIR.iterdir():\n",
    "            if sub.is_dir():\n",
    "                candidate = sub / fname\n",
    "                if candidate.exists():\n",
    "                    src = candidate\n",
    "                    break\n",
    "    if src.exists():\n",
    "        shutil.copy2(str(src), str(ONNX_DIR / fname))\n",
    "\n",
    "# Copy export_meta.json\n",
    "shutil.copy2(str(FINAL_MODEL_DIR / \"export_meta.json\"), str(ONNX_DIR / \"export_meta.json\"))\n",
    "\n",
    "print(f\"\\nğŸ“¦ All ONNX artefacts saved to: {ONNX_DIR}\")\n",
    "print(\"   Contents:\")\n",
    "for f in sorted(ONNX_DIR.iterdir()):\n",
    "    size = f.stat().st_size\n",
    "    print(f\"   {f.name:40s}  {size/1e6:.2f} MB\" if size > 1e5 else f\"   {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1f0b06",
   "metadata": {},
   "source": [
    "## 7. Validate ONNX Model\n",
    "\n",
    "Sanity-check: compare PyTorch vs ONNX outputs on a few sample texts to ensure export fidelity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e96b932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# â”€â”€ Load ONNX sessions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "sess_float = ort.InferenceSession(str(onnx_float_path))\n",
    "sess_int8  = ort.InferenceSession(str(onnx_int8_path))\n",
    "\n",
    "test_texts = [\n",
    "    \"Machine learning is a branch of artificial intelligence.\",\n",
    "    \"The Eiffel Tower is located in Paris, France.\",\n",
    "    \"Quantum computing uses qubits instead of classical bits.\",\n",
    "    \"Python is a popular programming language for data science.\",\n",
    "    \"Short query\",\n",
    "]\n",
    "\n",
    "# â”€â”€ PyTorch reference embeddings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "with torch.no_grad():\n",
    "    encoded = tokenizer(test_texts, padding=True, truncation=True,\n",
    "                        max_length=512, return_tensors=\"pt\")\n",
    "    out = model(encoded)\n",
    "    pt_embs = F.normalize(out[\"sentence_embedding\"], p=2, dim=-1).numpy()\n",
    "\n",
    "# â”€â”€ ONNX embeddings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def run_onnx(session, texts):\n",
    "    enc = tokenizer(texts, padding=True, truncation=True,\n",
    "                    max_length=512, return_tensors=\"np\")\n",
    "    feed = {\"input_ids\": enc[\"input_ids\"].astype(np.int64),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].astype(np.int64)}\n",
    "    # Add token_type_ids if model expects it\n",
    "    input_names = [i.name for i in session.get_inputs()]\n",
    "    if \"token_type_ids\" in input_names and \"token_type_ids\" in enc:\n",
    "        feed[\"token_type_ids\"] = enc[\"token_type_ids\"].astype(np.int64)\n",
    "    return session.run(None, feed)[0]\n",
    "\n",
    "onnx_float_embs = run_onnx(sess_float, test_texts)\n",
    "onnx_int8_embs  = run_onnx(sess_int8, test_texts)\n",
    "\n",
    "# â”€â”€ Compare â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"{'Text':<55} {'PTâ†”Float':>10} {'PTâ†”INT8':>10} {'Floatâ†”INT8':>10}\")\n",
    "print(\"â”€\" * 90)\n",
    "for i, txt in enumerate(test_texts):\n",
    "    cos_pf = np.dot(pt_embs[i], onnx_float_embs[i])\n",
    "    cos_pi = np.dot(pt_embs[i], onnx_int8_embs[i])\n",
    "    cos_fi = np.dot(onnx_float_embs[i], onnx_int8_embs[i])\n",
    "    print(f\"{txt[:53]:<55} {cos_pf:>10.6f} {cos_pi:>10.6f} {cos_fi:>10.6f}\")\n",
    "\n",
    "print(f\"\\nâœ… Mean PTâ†”Float cosine: {np.mean([np.dot(pt_embs[i], onnx_float_embs[i]) for i in range(len(test_texts))]):.6f}\")\n",
    "print(f\"âœ… Mean PTâ†”INT8  cosine: {np.mean([np.dot(pt_embs[i], onnx_int8_embs[i]) for i in range(len(test_texts))]):.6f}\")\n",
    "print(f\"âœ… Embedding dim: {onnx_float_embs.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd581a03",
   "metadata": {},
   "source": [
    "## 8. (Optional) Push to Hugging Face Hub\n",
    "\n",
    "Uncomment and fill in your HF token to push the model & ONNX artefacts to your Hub repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48703cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Uncomment below to push to HF Hub â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# from huggingface_hub import HfApi, login\n",
    "#\n",
    "# HF_TOKEN = \"hf_YOUR_TOKEN_HERE\"  # or use Colab secrets\n",
    "# REPO_ID  = \"your-username/mdbr-leaf-mt-1536-large-onnx\"\n",
    "#\n",
    "# login(token=HF_TOKEN)\n",
    "# api = HfApi()\n",
    "#\n",
    "# # Upload ONNX directory\n",
    "# api.upload_folder(\n",
    "#     folder_path=str(ONNX_DIR),\n",
    "#     repo_id=REPO_ID,\n",
    "#     repo_type=\"model\",\n",
    "#     commit_message=\"Upload distilled ONNX model (text-embedding-3-large, 1536d)\",\n",
    "# )\n",
    "#\n",
    "# # Upload final SentenceTransformer model\n",
    "# api.upload_folder(\n",
    "#     folder_path=str(FINAL_MODEL_DIR),\n",
    "#     repo_id=REPO_ID,\n",
    "#     path_in_repo=\"sentence_transformer\",\n",
    "#     repo_type=\"model\",\n",
    "#     commit_message=\"Upload SentenceTransformer checkpoint\",\n",
    "# )\n",
    "#\n",
    "# print(f\"âœ… Pushed to https://huggingface.co/{REPO_ID}\")\n",
    "\n",
    "print(\"â„¹ï¸  Uncomment the cell above and set your HF token to push to the Hub.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
